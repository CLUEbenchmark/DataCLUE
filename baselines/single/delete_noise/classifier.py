import os
os.environ["CUDA_VISIBLE_DEVICE"] = '1'
from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification, AutoTokenizer
import torch
import numpy as np
from datasets import load_metric
from sklearn.model_selection import StratifiedKFold  # StratifiedKFoldåˆ’åˆ†æ•°æ®é›†çš„åŸç†ï¼šåˆ’åˆ†åçš„è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸­ç±»åˆ«åˆ†å¸ƒå°½é‡å’ŒåŸæ•°æ®é›†ä¸€æ ·

path = os.path.split(os.path.split(os.path.realpath(__file__))[0])[0]
from baselines.single.data_aug.parallel_textda import parallel_expansion
os.environ["TOKENIZERS_PARALLELISM"] = 'false'
PRETRAIN = 'hfl/rbtl3'  # åŠ è½½çš„é¢„è®­ç»ƒæ¨¡å‹çš„åç§°
metric = load_metric("f1")  # ä½¿ç”¨f1 scoreä½œä¸ºæŒ‡æ ‡


# è®¡ç®—æ ‡ç­¾ä¸é¢„æµ‹å€¼åœ¨ç»™å®šçš„æŒ‡æ ‡ä¸Šçš„æ•ˆæœ
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return metric.compute(predictions=predictions, references=labels, average='macro')


class MyDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx])
                for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)


def get_prediction(data):
    """
    è®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼Œå¾—åˆ°æ•°æ®ç‚¹ä¸Šçš„æ ‡ç­¾é¢„æµ‹ï¼š
    1ï¼‰ åŠ è½½æ•°æ®ï¼›
    2ï¼‰ä½¿ç”¨KæŠ˜äº¤å‰éªŒè¯è®­ç»ƒï¼Œå¹¶åœ¨éªŒè¯é›†ä¸Šåšé¢„æµ‹ï¼›
    3) åˆå¹¶äº¤å‰éªŒè¯çš„ç»“æœï¼Œå¹¶å¾—åˆ°æ•´ä¸ªæ•°æ®é›†ä¸Šæ¨¡å‹çš„é¢„æµ‹çš„æ¦‚ç‡åˆ†å¸ƒ
    train a model to get estimation of each data point
    """
    # 1ã€åŠ è½½æ‰€æœ‰æ•°æ®ã€æ ‡ç­¾åˆ°åˆ—è¡¨ all_text, all_label,all_id
    all_text, all_label, all_id = [], [], []
    for idx, line in enumerate(data['json']):
        all_text.append(line['sentence'])
        all_label.append(int(line['label']))
        all_id.append(idx)
    # åŠ è½½æ ‡ç­¾å®šä¹‰å¢å¼ºåçš„æ•°æ®
    # label_data.json--->{"id": -1, "sentence": "ä¹°å®¶æŠ±æ€¨å•†å“äº†", "label_des": "ä¹°å®¶æŠ±æ€¨å•†å“æ¶¨ä»·äº†\n", "label": 0}
    label_text, label_label = [], []
    # for line in open('../../datasets/cic/label_data.json', 'r', encoding='utf-8'):
    #     label_text.append(json.loads(line)['sentence'])
    #     label_label.append(int(json.loads(line)['label']))

    # 2ã€ä½¿ç”¨KæŠ˜äº¤å‰éªŒè¯è®­ç»ƒï¼Œå¹¶åœ¨éªŒè¯é›†ä¸Šåšé¢„æµ‹ï¼šéå†æ¯ä¸€æŠ˜å¾—åˆ°è®­ç»ƒé›†å’ŒéªŒè¯å­é›†ã€æ•°æ®å¢å¼ºã€è®¾ç½®è®­ç»ƒå‚æ•°å’Œæ•°æ®è¿›è¡Œè®­ç»ƒã€åœ¨éªŒè¯é›†ä¸Šè¿›è¡Œé¢„æµ‹
    dev_out = {}  # å¸¦ç´¢å¼•(index)çš„éªŒè¯å­é›†çš„åˆ—è¡¨
    dev_index = {}  # å¸¦ç´¢å¼•(index)çš„éªŒè¯é›†çš„åˆ—è¡¨
    kf = StratifiedKFold(n_splits=6, shuffle=True)  # StratifiedKFoldåˆ’åˆ†æ•°æ®é›†çš„åŸç†ï¼šåˆ’åˆ†åçš„è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸­ç±»åˆ«åˆ†å¸ƒå°½é‡å’ŒåŸæ•°æ®é›†ä¸€æ ·
    # kf.get_n_splits(all_text, all_label)
    for kf_id, (train_index, test_index) in enumerate(kf.split(all_text, all_label)):
        # 2.1 å¾—åˆ°è®­ç»ƒå’ŒéªŒè¯å­é›†
        # kf_id:ç¬¬å‡ æŠ˜ï¼›train_index, test_indexè¿™ä¸€æŠ˜çš„è®­ç»ƒã€éªŒè¯é›†ã€‚
        train_text = [all_text[i] for i in train_index][:] + label_text  # è®­ç»ƒé›†çš„æ–‡æœ¬
        train_label = [all_label[i] for i in train_index][:] + label_label  # è®­ç»ƒé›†çš„æ ‡ç­¾
        dev_text = [all_text[i] for i in test_index]
        dev_label = [all_label[i] for i in test_index]
        dev_index[kf_id] = test_index

        # 2.2 å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œæ•°æ®æ‰©å¢
        # new_train_text = []
        # new_train_label = []
        # for idx, tmp_text in enumerate(train_text):
        #     sen_list = data_expansion(tmp_text, alpha_ri=0.1, alpha_rs=0, num_aug=5)
        #     new_train_text.extend(sen_list)
        #     new_train_label.extend([train_label[idx]] * len(sen_list))
        #
        # train_text = new_train_text
        # train_label = new_train_label

        sen_list, label_list = parallel_expansion(train_text, train_label, alpha_ri=0.1, alpha_rs=0, num_aug=5)
        train_text = sen_list
        train_label = label_list
        assert len(train_text) == len(train_label)
        # 2.3 è®¾ç½®ä½¿ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¹¶è®¾ç½®tokenizerã€æ•°æ®é›†å¯¹è±¡
        tokenizer = AutoTokenizer.from_pretrained(PRETRAIN, do_lower_case=True)
        train_encodings = tokenizer(train_text, truncation=True, padding=True, max_length=32)
        val_encodings = tokenizer(dev_text, truncation=True, padding=True, max_length=32)

        train_dataset = MyDataset(train_encodings, train_label)
        val_dataset = MyDataset(val_encodings, dev_label)

        # 2.4 å®ä¾‹åŒ–è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            # output directory
            output_dir='../../tmpresults/tmpresult{}'.format(kf_id),
            num_train_epochs=50,  # total number of training epochs
            per_device_train_batch_size=256,  # batch size per device during training
            per_device_eval_batch_size=32,  # batch size for evaluation
            warmup_steps=500,  # number of warmup steps for learning rate scheduler
            learning_rate=3e-4 if 'electra' in PRETRAIN else 2e-5,
            weight_decay=0.01,  # strength of weight decay
            save_total_limit=1,
            # logging_dir='../../tmplogs',  # directory for storing logs
            # logging_steps=10,
            # evaluation_strategy="epoch",
        )
        model = AutoModelForSequenceClassification.from_pretrained(PRETRAIN, num_labels=len(data['info']))

        # 2.5 åˆ©ç”¨å®ä¾‹åŒ–çš„è®­ç»ƒå¯¹è±¡è¿›è¡Œè®­ç»ƒï¼ˆæ¨¡å‹ã€è®­ç»ƒå‚æ•°ã€è®­ç»ƒé›†ã€éªŒè¯é›†ã€è¯„ä»·æŒ‡æ ‡ï¼‰
        trainer = Trainer(
            # the instantiated ğŸ¤— Transformers model to be trained
            model=model,
            args=training_args,  # training arguments, defined above
            train_dataset=train_dataset,  # training dataset
            eval_dataset=val_dataset,  # evaluation dataset
            compute_metrics=compute_metrics,
        )
        trainer.train()  # è®­ç»ƒæ¨¡å‹

        # 2.6 åˆ©ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šè¿›è¡Œé¢„æµ‹
        dev_outputs = trainer.predict(val_dataset).predictions
        dev_out[kf_id] = dev_outputs  # å°†é¢„æµ‹ç»“æœä¿å­˜åœ¨åˆ—è¡¨ä¸­

    # 3ã€åˆå¹¶äº¤å‰éªŒè¯çš„ç»“æœï¼Œå¹¶å¾—åˆ°æ•´ä¸ªæ•°æ®é›†ä¸Šæ¨¡å‹çš„é¢„æµ‹çš„æ¦‚ç‡åˆ†å¸ƒ
    alls = [0] * len(all_label)
    for kfid in range(6):
        for idx, item in enumerate(dev_index[kfid]):
            # dev_index[0]:ç¬¬0æŠ˜çš„éªŒè¯æ•°æ®çš„ç´¢å¼•çš„åˆ—è¡¨
            alls[item - 1] = dev_out[kfid][idx]
    outputs = np.array(alls)
    return outputs


if __name__ == '__main__':
    get_prediction()
